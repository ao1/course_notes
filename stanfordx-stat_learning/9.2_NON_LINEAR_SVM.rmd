## Non-Linear Support Vector Machine

* load data from web
* show variable names
* px1 and px2 are the grid values for each of the variables

```{r}
library(e1071)
load(url("http://www.stanford.edu/~hastie/ElemStatLearn/datasets/ESL.mixture.rda"))
names(ESL.mixture)
attach(ESL.mixture)
```

* plot the data, they seem to overlap a lot
* make a dataframe
* fit a svm model with kernel=radial, no scaling
* Normally we should select the cost parameter, could use cv for that

```{r}
plot(x,col=y+1)
dat = data.frame(y=factor(y),x)
fit=svm(factor(y)~.,data=dat,scale=F,kernel="radial",cost=5)
fit
```

* make predictions on the grid
* we can see the non-linear decision boundary
* decision boundaries mostly follow the data

```{r}
xgrid=expand.grid(X1=px1,X2=px2)
ygrid=predict(fit,xgrid)
plot(xgrid,col=as.numeric(ygrid),pch=20,cex=0.2)
points(x,col=y+1,pch=19)
```

* put in a curve that represents the decision boundary
* use the contour function for this
* prob gives the true probability of class 1 at the gridpoints
* contour 0.5 is the true decision boundary
* countour tracks the decision boundary
* blue contour is the true decision boundary aka bayes decision boundary
* if we had large amounts of data we'd hope to be able to get close to this, our svm has got pretty close to that

```{r}
func = predict(fit,xgrid,decision.values=T)
func = attributes(func)$decision
xgrid = expand.grid(X1=x1,X2=x2)
ygrid = predict(fit,xgrid)
plot(xgrid,col=as.numeric(ygrid),pch=20,cex=0.2)
points(x,col=y+1,pch=19)

contour(px1,px2,matrix(func,69,99),level=0,add=T)
contour(px1,px2,matrix(prob,69,99),level=0.5,add=T,col="blue",lwd=2)
```

