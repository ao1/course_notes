## Random Forest

* reduces variance of trees by averaging
* grows big bushy trees, gets rid of the variance by averaging

* import packages
* use Boston housing data from MASS library
* create random train set index from Boston
* create train and test sets

```{r}
par(mfrow=c(1,1))
Sys.setlocale("LC_ALL", "C")
set.seed(101)
require(ISLR)
require(randomForest)
require(MASS)
require(gbm)
names(Boston)
dim(Boston)
train_index = sample(1:nrow(Boston),300)
train = Boston[train_index,]
test = Boston[-train_index,]
```

* fit a random forest model
* show model details
* show number of trees used in model
* each observation was predicted using the average of trees that did not include it, these are de-biased estimates of prediction error
* the only tuning parameter in random forest is ntry
* ntry is the amount of variables selected at each split of each tree
* if ntry is 4, 4 vars are selected at random at each node split

```{r}
rf.boston =  randomForest(medv ~ . , data=train)
rf.boston
rf.boston$ntree
```

* we will try to fit a series of random forests and record errors
* for mtry=1:13, fit model that many times
  + extract the mean squared error
  + predict on test data
  + compute test error
  + compute mean squared error
  + print out mtry value, in case it runs slow

```{r}
oob_err = double(13)
test_err = double(13)

for(mtry in 1:13)
{
  fit = randomForest(medv~. , data=train , mtry=mtry , ntree=400)
  oob_err[mtry] = fit$mse[400]
  pred = predict(fit,test)
  test_err[mtry] = with(test,mean((medv-pred)^2))
  cat(mtry, " ")
}
```

* plot both errors in one plot
* the test error is lower, should be closer
* but the difference is well within standard error
* the 2 estimates are correlated
* mtry 4 seems lowest for the test error
* leftmost of plot is performance of a single tree (MSE improved by half at mtry=8)


```{r}
matplot(1:mtry,cbind(test_err,oob_err),pch=19,col=c("red","blue"),type="b",ylab="Mean Squared Error (lower is better)")
legend("topright",legend=c("OOB","Test"),pch=19,col=c("red","blue"))
```


## Boosting

* grows smaller, stubbier trees and goes at the bias

* use data sets
* gaussian distribution becase we're doing squared error loss
* ask for 10000 trees, seems like a lot but they are shallow
* interaction.depth = shallowness of trees
* summary gives a variable importance plot
* rm and lstat are most important variables
* reduce font size in plot to see all variable names

```{r}
boost.boston=gbm(medv~.,data=train,distribution="gaussian",n.trees=10000,shrinkage=0.01,interaction.depth=4)
summary(boost.boston)
```

* create partial dependence plots for the top 2 variables
* higher proportion of lower status people = lower value of house
* higher number of rooms = higher value of house

```{r}
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")

```

* make a vector of number of trees from 100 to 10000, 100 step
* make predictions on test set using boosted model
* this produced a matrix of predictions on the test data

```{r}
n.trees = seq(100,10000,100) # seq(from,to,step)
predmat = predict(boost.boston,newdata=test,n.trees=n.trees)
dim(predmat)
```

* compute test error for each of the predictions
* plot boosting error
* it drops down lower than the random forest
* boosting is reluctant to overfit, curve stays down
* boosting outperforms rf, but needs tuning with parameters
* rf better out of the box

```{r}
berr=with(test,apply((predmat-medv)^2,2,mean))

plot(n.trees,berr,pch=19,ylab="Mean Squared Error", xlab="# Trees",main="Boosting Test Error")
```











